# -*- coding: utf-8 -*-
"""train algorithims.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11h25GeByokrbKC5ZJFRxs3gMNNsF6PrP
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install --user pyLDAvis

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split


import string
import sklearn.metrics as metrics
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import plotly.offline as pyo
import plotly.graph_objects as go
from sklearn.feature_extraction.text import CountVectorizer

import matplotlib.pyplot as plt
import seaborn as sns
import re
from wordcloud import WordCloud, STOPWORDS
from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer
from sklearn.metrics import (accuracy_score, roc_auc_score, f1_score, hamming_loss,
                             classification_report, multilabel_confusion_matrix)

from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.multioutput import ClassifierChain
from sklearn.dummy import DummyClassifier


import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer

from google.colab import drive
drive.mount('/content/drive')

"""# Load & Explore Dataset"""

df= pd.read_csv('/content/drive/MyDrive/final project dataset /train.csv')

df

counts=[]
for i in df['comment_text'] :
  counts.append(len(i))

df['counts']=counts

df_cleaned=df[df['counts']<400]

toxic = df_cleaned[df_cleaned['toxic']== 1]



toxic



nontoxic = df[df['toxic']== 0][0:len(toxic)]

df = pd.concat([toxic[0:5000], nontoxic[0:5000]], axis=0)

df = df.sample(frac=1).reset_index(drop=True)

df

# List of column names to drop
columns_to_drop = ['id','counts']
#'severe_toxic', 'obscene', 'threat','insult','identity_hate'

# Dropping multiple columns
df.drop(columns=columns_to_drop, inplace=True)

df.info()

df.describe()

df

df.info()

COLUMNS = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']
df_distribution = df[COLUMNS].sum()\
                            .to_frame()\
                            .rename(columns={0: 'count'})\
                            .sort_values('count')

df_distribution.plot.pie(y='count',
                                      title='Label distribution over comments (without "none" category)',
                                      figsize=(5, 5))\
                            .legend(loc='center left', bbox_to_anchor=(1.3, 0.5))

f, ax = plt.subplots(figsize=(9, 6))
f.suptitle('Correlation matrix for categories')
sns.heatmap(df[COLUMNS].corr(), annot=True, linewidths=.5, ax=ax)

from matplotlib_venn import venn2
from matplotlib_venn import venn3

t = df[(df['toxic'] == 1) & (df['insult'] == 0) & (df['obscene'] == 0)].shape[0]
i = df[(df['toxic'] == 0) & (df['insult'] == 1) & (df['obscene'] == 0)].shape[0]
o = df[(df['toxic'] == 0) & (df['insult'] == 0) & (df['obscene'] == 1)].shape[0]

t_i = df[(df['toxic'] == 1) & (df['insult'] == 1) & (df['obscene'] == 0)].shape[0]
t_o = df[(df['toxic'] == 1) & (df['insult'] == 0) & (df['obscene'] == 1)].shape[0]
i_o = df[(df['toxic'] == 0) & (df['insult'] == 1) & (df['obscene'] == 1)].shape[0]

t_i_o = df[(df['toxic'] == 1) & (df['insult'] == 1) & (df['obscene'] == 1)].shape[0]


# Make the diagram
plt.figure(figsize=(8, 8))
plt.title("Venn diagram for 'toxic', 'insult' and 'obscene'")
venn3(subsets = (t, i, t_i, o, t_o, i_o, t_i_o),
      set_labels=('toxic', 'insult', 'obscene'))
plt.show()

t = df[(df['toxic'] == 1) & (df['severe_toxic'] == 0)].shape[0]
s = df[(df['toxic'] == 0) & (df['severe_toxic'] == 1)].shape[0]

t_s = df[(df['toxic'] == 1) & (df['severe_toxic'] == 1)].shape[0]


# Make the diagram
plt.figure(figsize=(8, 8))
plt.title("Venn diagram for 'toxic' and 'severe_toxic'")
venn2(subsets = (t, s, t_s),
      set_labels=('toxic', 'severe_toxic'))
plt.show()

"""## pre-processing"""

nltk.download('stopwords')

"""# Clean Text
def clean_text(text):
#replace the html characters with " "
    text=re.sub('<.*?>', ' ', text)  
#remove the punctuations
    text = text.translate(str.maketrans(' ',' ',string.punctuation))
#consider only alphabets and numerics
    text = re.sub('[^a-zA-Z]',' ',text)  
#replace newline with space
    text = re.sub("\n"," ",text)
#convert to lower case
    text = text.lower()
#split and join the words
    text=' '.join(text.split())
    return text

def stopwords(input_text, stop_words):
    word_tokens = word_tokenize(input_text)
    output_text = [w for w in word_tokens if not w in stop_words]
    output = []
    for w in word_tokens:
        if w not in stop_words:
            output.append(w)
            
    text = ' '.join(output)
    return text

unrelevant_words = ['wiki','wikipedia','page']
def clean(data,word):
    #Clean step 1, 2 and 3
    stop_words = set(nltk.corpus.stopwords.words('english'))
    data[word] = data[word].apply(lambda x: ''.join([w for w in clean_text(x) if w not in unrelevant_words]))
    #Clean Step 4
    data[word] = data[word].apply(lambda x: ''.join([w for w in stopwords(x,stop_words)]))
    #Clean Step 5
    data[word] = data[word].apply(lambda x: ''.join([w for w in lemmatizer.lemmatize(x)]))

# Clean Comment Text
clean(df,"comment_text")
df.head()
"""

def clean_text(text):
    # Remove HTML tags
    text = re.sub(r'<[^>]+>', '', text)

    # Remove web links
    text = re.sub(r'http\S+|www\S+|https\S+', '', text)

    # Remove special characters, punctuation marks, and numbers
    text = re.sub(r'[^a-zA-Z\s]', ' ', text)

    # Insert spaces between certain patterns (e.g., "ie", "eg")
    text = re.sub(r'(\s)([iI][eE]|[eE][gG])(\s)', r' \2 ', text)

    # Remove extra white spaces
    text = " ".join(text.split())

    return text.lower()

df['Cleaned_Comments'] = df['comment_text'].apply(clean_text)
df['Cleaned_Comments'].head()

import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords

def remove_stopwords(text):
    stop_words = set(stopwords.words('english'))
    words = text.split()  # Tokenize the text into words
    filtered_words = [word for word in words if word.lower() not in stop_words]  # Filter out stopwords
    return ' '.join(filtered_words)  # Join the words back into a sentence

df['Cleaned_Comments'] = df['Cleaned_Comments'].apply(remove_stopwords)
df['Cleaned_Comments'].head()



"""# Spliting Dataset"""

X = pd.DataFrame(df['Cleaned_Comments'])

X

df

# Target variables
Y = df.drop(['comment_text','Cleaned_Comments'],axis=1)

Y

"""# split"""

# X_first, x_second, y_first, y_seconnd = train_test_split(
#     X, Y, test_size=0.50, random_state=42,stratify=Y)
# Train_texts, Test_texts, Train_labels, Test_labels = train_test_split(
#     X_first, y_first, test_size=0.30, random_state=42,stratify=y_first)

# #validation set
# test_texts, val_texts, test_labels, val_labels = train_test_split(
#     Test_texts, Test_labels, test_size=0.3, random_state=42,stratify=Test_labels)
# print('Dataset for Baseline -->',len(x_second), x_second.shape)
# print('Training Dataset -->',len(Train_texts), Train_labels.shape)
# print('Testing Dataset -->',len(test_texts), test_labels.shape)
# print('Validation Dataset -->',len(val_texts), val_labels.shape)

X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=42)



X_train

y_train

df['toxic'].value_counts()

# Function to generate word cloud
def generate_wordcloud(text,Title):
    wordcloud = WordCloud(width=800, height=400,stopwords=set(STOPWORDS), background_color='black').generate(text)
    plt.figure(figsize=(10, 5))
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.axis("off")
    plt.title(Title)
    plt.show()

target_labels= [col for col in df.columns if df[col].dtypes == 'int64']
target_labels

import matplotlib.pyplot as plt
from wordcloud import WordCloud

# Function to generate word cloud
def generate_wordcloud(text, Title):
    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)
    plt.figure(figsize=(10, 5))
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.title(Title)
    plt.axis('off')
    plt.show()

# Assuming 'target_labels' and 'df' are defined properly

# Plot word clouds for each target label
for label in target_labels:
    text = ' '.join(df[df[label] == 1]['Cleaned_Comments'].values)
    generate_wordcloud(text, Title=label)

comments=df['Cleaned_Comments'].to_list()
comments[:5]

import plotly.express as px
import plotly.offline as pyo
import plotly.graph_objects as go
############################################################################################################

# Calculate the value counts for each target label
value_counts = df[target_labels].sum()

# Add the count for the "neither" category (where all target labels are 0)
value_counts['Good Comments'] =   value_counts.sum()-len(df)

fig = px.bar(x=value_counts.index,
             y=value_counts.values,
             color=value_counts.index,
             color_discrete_sequence=px.colors.qualitative.Dark24_r)

fig.update_layout(title='<b>Counts of Target Labels',
                  xaxis_title='Toxicity Labels',
                  yaxis_title='Counts',
                  template='plotly',
                  width=1000,   # Adjust the width here (sm50aller value)
                  height=600)  # Adjust the height here (smaller value)

# Show the bar chart
fig.show()

df

df_cleaned=df.drop('comment_text', axis=1)

df_cleaned.shape

df_cleaned.head()

df_cleaned.rename(columns={'Cleaned_Comments': 'comments'}, inplace=True)

df_cleaned

X

from sklearn.feature_extraction.text import CountVectorizer

#pd.DataFrame(df_cleaned)

# # Initialize the CountVectorizer
# vectorizer = CountVectorizer()


# # Fit and transform the text data
# bow_matrix = vectorizer.fit_transform(df['Cleaned_Comments'])

# # Convert the result into a DataFrame
# bow_df = pd.DataFrame(bow_matrix.toarray(), columns=vectorizer.get_feature_names_out())

# # Combine with the original DataFrame
# df_bow = pd.concat([df['toxic'], bow_df], axis=1)

# # transform the data to a BOW
# # if you get error in CountVectorizer' object has no attribute 'get_feature_names' ues the line with get_feature_names_out
# vect = CountVectorizer()
# X_BOW = vect.fit_transform(df['Cleaned_Comments'])
# count_array = X_BOW.toarray()
# df_BOW = pd.DataFrame(count_array,columns = vect.get_feature_names_out())
# df_BOW



######################

from matplotlib import pyplot as plt
import numpy as np


# Creating dataset
a = np.array(counts)

# Creating histogram
fig, ax = plt.subplots(figsize =(10, 7))
ax.hist(a, bins = range(0,2600))

# Show plot
plt.show()

X_train

y_train

X

"""# BOW"""

X_train['Cleaned_Comments']

# transform the data to a BOW
# if you get error in CountVectorizer' object has no attribute 'get_feature_names' ues the line with get_feature_names_out
vect = CountVectorizer()
X_BOW = vect.fit_transform(X_train['Cleaned_Comments'])
count_array = X_BOW.toarray()
df_comment_BOW = pd.DataFrame(count_array,columns = vect.get_feature_names_out())


X_BOW_test = vect.fit_transform(X_test['Cleaned_Comments'])

count_array_test = X_BOW_test.toarray()


df_comment_BOW_test = pd.DataFrame(count_array_test,columns = vect.get_feature_names_out())

X_train

X

word_vectorizer = TfidfVectorizer(max_features=10000)
word_vectorizer.fit(X['Cleaned_Comments'])
train_word_features = word_vectorizer.transform(X_train['Cleaned_Comments'])
df_comment_tf_idf_train = pd.DataFrame(train_word_features.toarray(),columns=word_vectorizer.get_feature_names_out())
test_word_features = word_vectorizer.transform(X_test['Cleaned_Comments'])
df_comment_tf_idf_test= pd.DataFrame(test_word_features.toarray(),columns=word_vectorizer.get_feature_names_out())
df_comment_tf_idf_train

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import cross_val_score
from scipy.sparse import hstack

class_names = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']

print(df_comment_tf_idf_train)

df_comment_tf_idf_test

"""# Logistic regrassion"""

import numpy as np
from sklearn.datasets import make_classification
from sklearn import svm
from sklearn.metrics import accuracy_score
import pickle


submission = {}
scores_LR_tfif_test = []
scores_LR_tfif_train = []
for class_name in class_names:
    train_target = y_train[class_name]
    test_target = y_test[class_name]

    classifier = LogisticRegression(C=0.1, solver='sag')
    cv_score = np.mean(cross_val_score(classifier, df_comment_tf_idf_train, train_target, cv=3, scoring='roc_auc'))
    scores_LR_tfif_train.append(cv_score)
    print('CV score for class {} is {}'.format(class_name, cv_score))
    classifier.fit(df_comment_tf_idf_train, train_target)
    submission[class_name] = classifier.predict(df_comment_tf_idf_test)
    temp=accuracy_score(submission[class_name], test_target)
    scores_LR_tfif_test.append(temp)
    pickle.dump(classifier,open( f'model_LR_{class_name}.pkl',"wb"))

np.reshape(scores_LR_tfif_test,(-1,1))

scores_LR_tfif_train

classes= ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']

# Assuming you have the data for the scores in the variables socre_svm_itidf_trian and socre_svm_itidf_test


plt.plot(classes, scores_LR_tfif_train, color='r', label='LR itidf trian', marker='o')
plt.plot(classes, scores_LR_tfif_test, color='b', label='LR itidf test', marker='o')
plt.xlim(-0.5, 5.5)  # Set the x-axis limits appropriately
plt.legend()
plt.show()



train_target

submission

"""# SVM"""

import numpy as np
from sklearn.datasets import make_classification
from sklearn import svm
from sklearn.metrics import accuracy_score

socre_svm_itidf_trian=[]
socre_svm_itidf_test=[]


for class_name in class_names:
    train_target = y_train[class_name]
    test_target =y_test[class_name]
    model = svm.SVC(kernel = 'rbf', random_state = 10)
    #%%
    model.fit(df_comment_tf_idf_train,train_target)
    #%%
    y=model.predict(df_comment_tf_idf_train)
    y2=model.predict(df_comment_tf_idf_test)
    #%%

    score =accuracy_score(y, train_target)
    print(score)
    score2 =accuracy_score(y2, test_target)
    print(score2)
    socre_svm_itidf_trian.append(score)
    socre_svm_itidf_test.append(score2)
    pickle.dump(classifier,open( f'model_SVM_{class_name}.pkl',"wb"))

socre_svm_itidf_test

socre_svm_itidf_trian

# Assuming you have the data for the scores in the variables socre_svm_itidf_trian and socre_svm_itidf_test
classes= ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']

# Assuming you have the data for the scores in the variables socre_svm_itidf_trian and socre_svm_itidf_test


plt.plot(classes, socre_svm_itidf_trian, color='r', label='SVM itidf trian', marker='o')
plt.plot(classes, socre_svm_itidf_test, color='b', label='SVM itidf test', marker='o')
plt.xlim(-0.5, 5.5)  # Set the x-axis limits appropriately
plt.legend()
plt.show()

df_comment_tf_idf_train

train_target

"""# XGBoot"""



# First XGBoost model for Pima Indians dataset
from numpy import loadtxt
from xgboost import XGBClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
import pickle
socre_XGBoot_itidf_trian=[]
socre_XGBoot_itidf_test=[]

for class_name in class_names:
    train_target = y_train[class_name]
    test_target =y_test[class_name]
    # fit model no training data
    model = XGBClassifier()
    model.fit(df_comment_tf_idf_train, train_target)
    y_pred = model.predict(df_comment_tf_idf_train)
    predictions = [round(value) for value in y_pred]
    accuracy = accuracy_score(train_target, predictions)
    socre_XGBoot_itidf_trian.append(accuracy)
    # make predictions for test data
    y_pred = model.predict(df_comment_tf_idf_test)
    predictions = [round(value) for value in y_pred]

    # evaluate predictions
    accuracy = accuracy_score(test_target, predictions)
    socre_XGBoot_itidf_test.append(accuracy)
    print("Accuracy: %.2f%%" % (accuracy * 100.0))
    pickle.dump(model,open( f'model_XGBoot_{class_name}.pkl',"wb"))

y_train

import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix
for class_name in class_names:
    test_target =y_test[class_name]

    y_pred = model.predict(df_comment_tf_idf_test)
    predictions = [round(value) for value in y_pred]

    cm = confusion_matrix(test_target, predictions)
    ax= plt.subplot()
    sns.heatmap(cm, annot=True, fmt='g', ax=ax);  #annot=True to annotate cells, ftm='g' to disable scientific notation

    # labels, title and ticks
    ax.set_xlabel('Predicted labels');ax.set_ylabel('True labels');
    ax.set_title(f'Confusion Matrix XGBoost  {class_name}');
    plt.show()



plt.plot(classes, socre_XGBoot_itidf_trian, color='r', label='XGBOOT itidf trian', marker='o')
plt.plot(classes, socre_XGBoot_itidf_test, color='b', label='XGBOOT itidf test', marker='o')
plt.xlim(-0.5, 5.5)  # Set the x-axis limits appropriately
plt.legend()
plt.show()



"""# Clustering"""

from sklearn.manifold import TSNE
from yellowbrick.text import TSNEVisualizer

from gensim import models
from gensim.models import Word2Vec
from gensim import corpora, models, similarities
from sklearn.decomposition import LatentDirichletAllocation

from collections import Counter
from scipy.sparse import csr_matrix
from scipy.cluster.hierarchy import dendrogram, linkage
from sklearn.metrics import cohen_kappa_score
from sklearn.metrics.cluster import v_measure_score

from gensim.models.coherencemodel import CoherenceModel
from gensim.models import CoherenceModel
from sklearn.metrics import silhouette_score
from yellowbrick.cluster import SilhouetteVisualizer
import pyLDAvis.gensim_models as genisvis
from sklearn.cluster import KMeans
from numpy import concatenate

"""# set of TF IDF"""

tsne = TSNEVisualizer()
tsne.fit(df_comment_tf_idf_train,y_train['toxic'])
tsne.show()



tsne = TSNEVisualizer()
tsne.fit(df_comment_tf_idf_train,y_train['severe_toxic'])
tsne.show()

tsne = TSNEVisualizer()
tsne.fit(df_comment_tf_idf_train,y_train['obscene'])
tsne.show()

tsne = TSNEVisualizer()
tsne.fit(df_comment_tf_idf_train,y_train['threat'])
tsne.show()

tsne = TSNEVisualizer()
tsne.fit(df_comment_tf_idf_train,y_train['insult'])
tsne.show()

tsne = TSNEVisualizer()
tsne.fit(df_comment_tf_idf_train,y_train['identity_hate'])
tsne.show()

"""# set of Ngram"""

#N_gram
count_vect = CountVectorizer(ngram_range=(1,2),max_features=10000)
ngram_vectors=count_vect.fit_transform(df['Cleaned_Comments'])
# if you get error in CountVectorizer' object has no attribute 'get_feature_names' ues the line with get_feature_names_out
#df_books_ngram = pd.DataFrame(ngram_vectors.toarray(),columns=count_vect.get_feature_names())
df_comment_ngram = pd.DataFrame(ngram_vectors.toarray(),columns=count_vect.get_feature_names_out())
df_comment_ngram

['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']

tsne = TSNEVisualizer()
tsne.fit(df_comment_ngram,y_train['toxic'])
plt.show()

tsne = TSNEVisualizer()
tsne.fit(df_comment_ngram,y_train['severe_toxic'])
plt.show()

tsne = TSNEVisualizer()
tsne.fit(df_comment_ngram,y_train['obscene'])
plt.show()

tsne = TSNEVisualizer()
tsne.fit(df_comment_ngram,y_train['threat'])
plt.show()

tsne = TSNEVisualizer()
tsne.fit(df_comment_ngram,y_train['insult'])
plt.show()

tsne = TSNEVisualizer()
tsne.fit(df_comment_ngram,y_train['identity_hate'])
plt.show()

# create a dictionary of the book words
import gensim

wordsDictionary = corpora.Dictionary(df['Cleaned_Comments'].str.split())
corpus = [wordsDictionary.doc2bow(wordDic) for wordDic in df['Cleaned_Comments'].str.split()]


LDAModel = gensim.models.ldamodel.LdaModel(corpus = corpus,
                                           id2word = wordsDictionary,
                                           num_topics = 5,
                                           random_state = 42,
                                           update_every = 1,
                                           chunksize = 100,
                                           passes = 10,
                                           alpha = 'auto',
                                           per_word_topics = True)
PredictedWords = LDAModel.inference(corpus)

import gensim
import pyLDAvis
pyLDAvis.enable_notebook()
visualization = genisvis.prepare(LDAModel, corpus, wordsDictionary)
visualization

def TSNEData(data):
  Tsna = TSNE(n_components= 2, random_state= 42)
  DataOfTSNE = Tsna.fit_transform(data)
  return DataOfTSNE

DataOfBOW = TSNEData(df_comment_BOW)
DataOfTFIDF = TSNEData( df_comment_tf_idf_train)

# function to visualize the five clusters
def Visualize_Clusters(ClusteringModel, result, Y_Prediction,n, em=True,title=''):
    # get centroids of kmeans cluster
    if em:
        centroids = np.empty(shape=(ClusteringModel.n_components, result.shape[1]))
    else:
        centroids = ClusteringModel.cluster_centers_

  # we want to transform the rows and the centroids
  # todense return matrix
    matrix_data = csr_matrix(result)
    all_Data = concatenate((matrix_data.todense(), centroids))


    n_clusters = n


    plt.scatter([all_Data[:-n_clusters, 0]], [all_Data[:-n_clusters, 1]], c=Y_Prediction, cmap=plt.cm.Paired, marker= 'x')
    plt.scatter([all_Data[-n_clusters:, 0]], [all_Data[-n_clusters:, 1]], marker= 'o')

    # Set the x-axis label
    plt.xlabel('X-Label')
    # Set the y-axis label
    plt.ylabel('Y-Label')
    # Set the title
    plt.title(title)
    plt.show()

# function to apply kmeans algorithm
def KMeansAlgorithm(clusters, X_data):
    kmeans = KMeans(n_clusters=clusters, random_state=42)
    Y_Prediction = kmeans.fit_predict(X_data)
    return kmeans, Y_Prediction

# function to display histogram of each cluster
def Clusters_Count(predictedClusters,title=''):
    clustersNumber = Counter(predictedClusters)
    plt.bar(clustersNumber.keys(), clustersNumber.values())
    plt.xlabel('Cluster Number')
    plt.ylabel('Count')
    # Set the title
    plt.title(title)
    plt.show()

# Bow with KMeans algorithm
silhouette_score_BOW=[]
for i in range (2,9):
  KMeansWithBOW, KMeansWithBOWPrediction = KMeansAlgorithm(i,DataOfBOW)
  score = silhouette_score(DataOfBOW, KMeansWithBOWPrediction)
  print("For n_clusters = {}, silhouette score is {})".format(i, score))
  silhouette_score_BOW.append(score)
  # visualize clusters
  Visualize_Clusters(KMeansWithBOW, DataOfBOW, KMeansWithBOWPrediction,i,False,title='BOW with KMeans algorithm TSNE',)

import matplotlib.pyplot as plt
x = range (2,9)
y = silhouette_score_BOW
plt.xlabel("X-axis")
plt.ylabel("Y-axis")
plt.title("A test graph")
plt.plot(x,y,label = 'id %s'%i)
plt.legend()
plt.show()

# TFIDF with KMeans algorithm
silhouette_score_BOW=[]
for i in range (2,9):
  KMeansWithBOW, KMeansWithBOWPrediction = KMeansAlgorithm(i,DataOfTFIDF)
  score = silhouette_score(DataOfTFIDF, KMeansWithBOWPrediction)
  print("For n_clusters = {}, silhouette score is {})".format(i, score))
  silhouette_score_BOW.append(score)
  # visualize clusters
  Visualize_Clusters(KMeansWithBOW, DataOfTFIDF, KMeansWithBOWPrediction,i,False,title='Tfidf with KMeans algorithm TSNE',)

import matplotlib.pyplot as plt
x = range (2,9)
y = silhouette_score_BOW
plt.xlabel("X-axis")
plt.ylabel("Y-axis")
plt.title("A test graph")
plt.plot(x,y,label = 'id %s'%i)
plt.legend()
plt.show()

def plot_dendrogram(model, **kwargs):
    # Create linkage matrix and then plot the dendrogram

    # create the counts of samples under each node
    counts = np.zeros(model.children_.shape[0])
    n_samples = len(model.labels_)
    for i, merge in enumerate(model.children_):
        current_count = 0
        for child_idx in merge:
            if child_idx < n_samples:
                current_count += 1  # leaf node
            else:
                current_count += counts[child_idx - n_samples]
        counts[i] = current_count

    linkage_matrix = np.column_stack(
        [model.children_, model.distances_, counts]
    ).astype(float)

    # Plot the corresponding dendrogram
    dendrogram(linkage_matrix, **kwargs)

from sklearn.cluster import AgglomerativeClustering
import numpy as np
from scipy.cluster.hierarchy import dendrogram


clustering = AgglomerativeClustering(distance_threshold=0, n_clusters=None).fit(DataOfTFIDF)

AgglomerativeClustering()
clustering.labels_

from sklearn.cluster import AgglomerativeClustering
import numpy as np
from scipy.cluster.hierarchy import dendrogram


clustering = AgglomerativeClustering(distance_threshold=0, n_clusters=None).fit(DataOfBOW)

AgglomerativeClustering()
clustering.labels_